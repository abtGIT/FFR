{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import face_recognition\n",
    "import argparse\n",
    "import imutils\n",
    "import pickle\n",
    "import time\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceRecognize:\n",
    "    \n",
    "    def __init__(self, encodings, input_file):\n",
    "        # load the known faces and embeddings\n",
    "        print(\"[INFO] loading encodings...\")\n",
    "        self.data = pickle.loads(open(encodings, \"rb\").read())\n",
    "        # initialize the pointer to the video file and the video self.writer\n",
    "        print(\"[INFO] processing video...\")\n",
    "        self.stream = cv2.VideoCapture(input_file)\n",
    "        self.writer = None\n",
    "        \n",
    "    def compareEncoding(self, data, encodings):\n",
    "        #print('compareEncoding(self, data, encodings):')\n",
    "        names = []\n",
    "        # loop over the facial embeddings\n",
    "        for encoding in encodings:\n",
    "            # attempt to match each face in the input image to our known\n",
    "            # encodings\n",
    "            matches = face_recognition.compare_faces(data[\"encodings\"], encoding)\n",
    "            name = \"Unknown\"\n",
    "\n",
    "            # check to see if we have found a match\n",
    "            if True in matches:\n",
    "                # find the indexes of all matched faces then initialize a\n",
    "                # dictionary to count the total number of times each face\n",
    "                # was matched\n",
    "                matchedIdxs = [i for (i, b) in enumerate(matches) if b]\n",
    "                counts = {}\n",
    "\n",
    "                # loop over the matched indexes and maintain a count for\n",
    "                # each recognized face face\n",
    "                for i in matchedIdxs:\n",
    "                    name = data[\"names\"][i]\n",
    "                    counts[name] = counts.get(name, 0) + 1\n",
    "\n",
    "                # determine the recognized face with the largest number\n",
    "                # of votes (note: in the event of an unlikely tie Python\n",
    "                # will select first entry in the dictionary)\n",
    "                name = max(counts, key=counts.get)\n",
    "            # update the list of names\n",
    "            names.append(name)\n",
    "        return names\n",
    "    \n",
    "    def drawBox(self, boxes, names, frame, r):\n",
    "        #print('drawBox(self, boxes, names):')\n",
    "         # loop over the recognized faces\n",
    "        for ((top, right, bottom, left), name) in zip(boxes, names):\n",
    "            # rescale the face coordinates\n",
    "            top = int(top * r)\n",
    "            right = int(right * r)\n",
    "            bottom = int(bottom * r)\n",
    "            left = int(left * r)\n",
    "\n",
    "            # draw the predicted face name on the image\n",
    "            cv2.rectangle(frame, (left, top), (right, bottom),(0, 255, 0), 2)\n",
    "            y = top - 15 if top - 15 > 15 else top + 15\n",
    "            cv2.putText(frame, name, (left, y), cv2.FONT_HERSHEY_SIMPLEX,0.75, (0, 255, 0), 2)\n",
    "\n",
    "    def writeVideo(self, output, frame):\n",
    "        #print('writeVideo(self, output):')\n",
    "        # if the video self.writer is None *AND* we are supposed to write\n",
    "        # the output video to disk initialize the self.writer\n",
    "        if self.writer is None and output is not None:\n",
    "            fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "            self.writer = cv2.VideoWriter(output, fourcc, 24, (frame.shape[1], frame.shape[0]), True)\n",
    "            \n",
    "        # if the self.writer is not None, write the frame with recognized\n",
    "        # faces t odisk\n",
    "        if self.writer is not None:\n",
    "            self.writer.write(frame)\n",
    "            \n",
    "    def ifDisplay(self, display, frame):\n",
    "        #print('ifDisplay(self, display):')\n",
    "        # check to see if we are supposed to display the output frame to\n",
    "        # the screen\n",
    "        key = None\n",
    "        if display > 0:\n",
    "            cv2.imshow(\"Frame\", frame)\n",
    "            key = cv2.waitKey(20) & 0xFF\n",
    "\n",
    "        return key\n",
    "\n",
    "    \n",
    "    def recognizeFace(self, detection_method, output, display):\n",
    "        # loop over frames from the video file stream\n",
    "        while True:\n",
    "            # grab the next frame\n",
    "            (grabbed, frame) = self.stream.read()\n",
    "\n",
    "            # if the frame was not grabbed, then we have reached the\n",
    "            # end of the stream\n",
    "            if not grabbed:\n",
    "                print('breaking')\n",
    "                break\n",
    "\n",
    "            # convert the input frame from BGR to RGB then resize it to have\n",
    "            # a width of 750px (to speedup processing)\n",
    "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            rgb = imutils.resize(frame, width=750)\n",
    "            print(rgb.shape)\n",
    "            r = frame.shape[1] / float(rgb.shape[1])\n",
    "\n",
    "            # detect the (x, y)-coordinates of the bounding boxes\n",
    "            # corresponding to each face in the input frame, then compute\n",
    "            # the facial embeddings for each face\n",
    "            boxes = face_recognition.face_locations(rgb, model=detection_method)\n",
    "            encodings = face_recognition.face_encodings(rgb, boxes)\n",
    "            names = self.compareEncoding(self.data, encodings)\n",
    "            self.drawBox(boxes, names, frame, r)\n",
    "            self.writeVideo(output, frame) #\n",
    "            key = self.ifDisplay(display, frame) # if we are supposed to display the output frame\n",
    "            # if the `q` key was pressed, break from the loop\n",
    "            if key == ord(\"q\"):\n",
    "                break\n",
    "            \n",
    "\n",
    "        # close the video file pointers\n",
    "        self.stream.release()\n",
    "\n",
    "        # check to see if the video self.writer point needs to be released\n",
    "        if self.writer is not None:\n",
    "            self.writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading encodings...\n",
      "[INFO] processing video...\n"
     ]
    }
   ],
   "source": [
    "encodings = '/home/dai/Documents/pgdai/project/project Data/cnn_encoding.pickle'\n",
    "input_file='/home/dai/Documents/pgdai/project/project Data/temp_test_data/2.mkv'\n",
    "detection_method = 'cnn'\n",
    "output = '/home/dai/Documents/pgdai/project/project Data/temp_output_data/out3.avi'\n",
    "display=0\n",
    "obj = FaceRecognize(encodings, input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breaking\n"
     ]
    }
   ],
   "source": [
    "obj.recognizeFace(detection_method, output, display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python recognize_faces_video_file.py --encodings encodings.pickle --input videos/lunch_scene.mp4\n",
    "# python recognize_faces_video_file.py --encodings encodings.pickle --input videos/lunch_scene.mp4 --output output/lunch_scene_output.avi --display 0\n",
    "# construct the argument parser and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-e\", \"--encodings\", required=True,\n",
    "\thelp=\"path to serialized db of facial encodings\")\n",
    "ap.add_argument(\"-i\", \"--input\", required=True,\n",
    "\thelp=\"path to input video\")\n",
    "ap.add_argument(\"-o\", \"--output\", type=str,\n",
    "\thelp=\"path to output video\")\n",
    "ap.add_argument(\"-y\", \"--display\", type=int, default=1,\n",
    "\thelp=\"whether or not to display output frame to screen\")\n",
    "ap.add_argument(\"-d\", \"--detection-method\", type=str, default=\"cnn\",\n",
    "\thelp=\"face detection model to use: either `hog` or `cnn`\")\n",
    "args = vars(ap.parse_args())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
